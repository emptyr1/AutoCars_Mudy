{
 "metadata": {
  "name": "",
  "signature": "sha256:36702449e9cf4b9165e445c56faa19fa2680677dce0cffd009c759ed4ab744f0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "------ TESTING BEAUTIFUL SOUP ON A SAMPLE PAGE --------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep\n",
      "BASE_URL = \"http://www.chicagoreader.com\"\n",
      "def make_soup(url):\n",
      "    html = urlopen(url).read()\n",
      "    return BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "def get_category_links(section_url):\n",
      "    \n",
      "    soup = make_soup(section_url)\n",
      "    boccat = soup.find(\"dl\", \"boccat\")\n",
      "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
      "    print category_links\n",
      "    return category_links"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_category_winner(category_url):\n",
      "    \n",
      "    soup = make_soup(category_url)\n",
      "    category = soup.find(\"h1\", \"headline\").string\n",
      "    winner = [h2.string for h2 in soup.findAll(\"h2\", \"boc1\")]\n",
      "    runners_up = [h2.string for h2 in soup.findAll(\"h2\", \"boc2\")]\n",
      "    return {\"category\": category,\n",
      "            \"category_url\": category_url,\n",
      "            \"winner\": winner,\n",
      "            \"runners_up\": runners_up}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == '__main__':\n",
      "    food_n_drink = (\"http://www.chicagoreader.com/chicago/\"\n",
      "                    \"best-of-chicago-2011-food-drink/BestOf?oid=4106228\")\n",
      "    \n",
      "    categories = get_category_links(food_n_drink)\n",
      " \n",
      "    data = [] # a list to store our dictionaries\n",
      "    for category in categories:\n",
      "        winner = get_category_winner(category)\n",
      "        data.append(winner)\n",
      "        sleep(0.2) # be nice on the servers plus you don't want them to know you're a scraping robot :) \n",
      " \n",
      "    #print data[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['http://www.chicagoreader.com/chicago/BestOf?category=1979894&year=2011', 'http://www.chicagoreader.com/chicago/best-fancy-restaurant-in-chicago/BestOf?oid=4088017', 'http://www.chicagoreader.com/chicago/best-bang-for-your-buck/BestOf?oid=4088018', 'http://www.chicagoreader.com/chicago/best-chef/BestOf?oid=4088191', 'http://www.chicagoreader.com/chicago/best-up-and-coming-chef/BestOf?oid=4088225', 'http://www.chicagoreader.com/chicago/best-food-blog/BestOf?oid=4088227', 'http://www.chicagoreader.com/chicago/best-ampersand-restaurant/BestOf?oid=4088228', 'http://www.chicagoreader.com/chicago/best-restaurant-name/BestOf?oid=4088229', 'http://www.chicagoreader.com/chicago/best-new-food-trend/BestOf?oid=4088231', 'http://www.chicagoreader.com/chicago/best-cocktail-list/BestOf?oid=4088236', 'http://www.chicagoreader.com/chicago/best-mixologist/BestOf?oid=4088461', 'http://www.chicagoreader.com/chicago/best-wine-list/BestOf?oid=4088462', 'http://www.chicagoreader.com/chicago/best-sommelier/BestOf?oid=4088464', 'http://www.chicagoreader.com/chicago/best-brewpub/BestOf?oid=4088465', 'http://www.chicagoreader.com/chicago/best-local-brew/BestOf?oid=4088468', 'http://www.chicagoreader.com/chicago/best-wine-shop/BestOf?oid=4088469', 'http://www.chicagoreader.com/chicago/best-liquor-store/BestOf?oid=4088470', 'http://www.chicagoreader.com/chicago/best-byob/BestOf?oid=4088471', 'http://www.chicagoreader.com/chicago/best-alfresco-dining/BestOf?oid=4088472', 'http://www.chicagoreader.com/chicago/best-late-night/BestOf?oid=4088473', 'http://www.chicagoreader.com/chicago/best-for-kids/BestOf?oid=4088474', 'http://www.chicagoreader.com/chicago/best-waitstaff/BestOf?oid=4088476', 'http://www.chicagoreader.com/chicago/best-looking-waitstaff/BestOf?oid=4088477', 'http://www.chicagoreader.com/chicago/best-food-festival/BestOf?oid=4088478', 'http://www.chicagoreader.com/chicago/best-food-truck/BestOf?oid=4088479', 'http://www.chicagoreader.com/chicago/best-gourmet-market/BestOf?oid=4088529', 'http://www.chicagoreader.com/chicago/best-local-grocer/BestOf?oid=4088661', 'http://www.chicagoreader.com/chicago/best-local-food-product/BestOf?oid=4088662', 'http://www.chicagoreader.com/chicago/best-farmers-market/BestOf?oid=4088668', 'http://www.chicagoreader.com/chicago/best-butcher-shop/BestOf?oid=4088669', 'http://www.chicagoreader.com/chicago/best-cheesemonger/BestOf?oid=4088670', 'http://www.chicagoreader.com/chicago/best-barista/BestOf?oid=4088671', 'http://www.chicagoreader.com/chicago/best-restaurant-with-a-view/BestOf?oid=4088672', 'http://www.chicagoreader.com/chicago/best-restaurant-for-romance/BestOf?oid=4088674', 'http://www.chicagoreader.com/chicago/best-restaurant-for-a-cheap-date/BestOf?oid=4088679', 'http://www.chicagoreader.com/chicago/best-neighborhood-restaurant/BestOf?oid=4088680', 'http://www.chicagoreader.com/chicago/best-bagels/BestOf?oid=4088681', 'http://www.chicagoreader.com/chicago/best-bakery/BestOf?oid=4088697', 'http://www.chicagoreader.com/chicago/best-barbecue/BestOf?oid=4088858', 'http://www.chicagoreader.com/chicago/best-breakfast/BestOf?oid=4088859', 'http://www.chicagoreader.com/chicago/best-brunch/BestOf?oid=4088860', 'http://www.chicagoreader.com/chicago/best-burger/BestOf?oid=4088863', 'http://www.chicagoreader.com/chicago/best-chinese/BestOf?oid=4088864', 'http://www.chicagoreader.com/chicago/best-coffee-shop/BestOf?oid=4088867', 'http://www.chicagoreader.com/chicago/best-desserts/BestOf?oid=4088868', 'http://www.chicagoreader.com/chicago/best-greek/BestOf?oid=4088869', 'http://www.chicagoreader.com/chicago/best-hot-dog/BestOf?oid=4088870', 'http://www.chicagoreader.com/chicago/best-ice-cream/BestOf?oid=4088872', 'http://www.chicagoreader.com/chicago/best-indian-restaurant/BestOf?oid=4088873', 'http://www.chicagoreader.com/chicago/best-italian/BestOf?oid=4088875', 'http://www.chicagoreader.com/chicago/best-korean/BestOf?oid=4088876', 'http://www.chicagoreader.com/chicago/best-mexican/BestOf?oid=4092029', 'http://www.chicagoreader.com/chicago/best-middle-eastern/BestOf?oid=4092030', 'http://www.chicagoreader.com/chicago/best-pizza/BestOf?oid=4092032', 'http://www.chicagoreader.com/chicago/best-polish/BestOf?oid=4092033', 'http://www.chicagoreader.com/chicago/best-pub-grub/BestOf?oid=4092035', 'http://www.chicagoreader.com/chicago/best-sandwiches/BestOf?oid=4092038', 'http://www.chicagoreader.com/chicago/best-seafood-restaurant/BestOf?oid=4092160', 'http://www.chicagoreader.com/chicago/best-steak-house/BestOf?oid=4092253', 'http://www.chicagoreader.com/chicago/best-sushi/BestOf?oid=4092254', 'http://www.chicagoreader.com/chicago/best-taqueria/BestOf?oid=4092259', 'http://www.chicagoreader.com/chicago/best-thai/BestOf?oid=4092260', 'http://www.chicagoreader.com/chicago/best-vegetarian/BestOf?oid=4092261', 'http://www.chicagoreader.com/chicago/best-vietnamese/BestOf?oid=4092262', 'http://www.chicagoreader.com/chicago/best-italian-steak-house-where-my-dad-felt-at-home-in-the-60s-and-i-do-nowp/BestOf?oid=4101104', 'http://www.chicagoreader.com/chicago/best-case-of-nostalgia-bordering-on-time-travel/BestOf?oid=4101105', 'http://www.chicagoreader.com/chicago/best-restaurant-empire-founded-the-same-year-as-the-reader/BestOf?oid=4101106', 'http://www.chicagoreader.com/chicago/best-restaurant/BestOf?oid=4101107', 'http://www.chicagoreader.com/chicago/best-bargain-michelin-chef/BestOf?oid=4101108', 'http://www.chicagoreader.com/chicago/best-chef-downshift-animal-division/BestOf?oid=4101109', 'http://www.chicagoreader.com/chicago/best-chef-downshift-vegetable-division/BestOf?oid=4101385', 'http://www.chicagoreader.com/chicago/best-venerable-restaurant-alongside-the-el/BestOf?oid=4101386', 'http://www.chicagoreader.com/chicago/best-new-food-truckfood/BestOf?oid=4101387', 'http://www.chicagoreader.com/chicago/best-buffet/BestOf?oid=4101388', 'http://www.chicagoreader.com/chicago/best-game-day/BestOf?oid=4101389', 'http://www.chicagoreader.com/chicago/best-dairy-product-to-camp-out-in-front-of-the-cheese-shop-for/BestOf?oid=4101390', 'http://www.chicagoreader.com/chicago/best-use-of-alcohol-at-breakfast/BestOf?oid=4101391', 'http://www.chicagoreader.com/chicago/best-university-coffeehouse/BestOf?oid=4101393', 'http://www.chicagoreader.com/chicago/best-bakery-youve-never-heard-of/BestOf?oid=4101397', 'http://www.chicagoreader.com/chicago/best-place-to-see-bakers-at-work/BestOf?oid=4101398', 'http://www.chicagoreader.com/chicago/best-place-for-ambience-and-egg-sandwiches/BestOf?oid=4101399', 'http://www.chicagoreader.com/chicago/best-bagel/BestOf?oid=4101410', 'http://www.chicagoreader.com/chicago/best-tubular-collaboration/BestOf?oid=4101411', 'http://www.chicagoreader.com/chicago/best-sausage/BestOf?oid=4101412', 'http://www.chicagoreader.com/chicago/best-place-in-chicago-to-sample-salumi-from-mario-batalis-papa/BestOf?oid=4101413', 'http://www.chicagoreader.com/chicago/best-broccoli-and-shells-con-patio/BestOf?oid=4101414', 'http://www.chicagoreader.com/chicago/best-fancy-pants-pizza-special/BestOf?oid=4101415', 'http://www.chicagoreader.com/chicago/best-polish-mexican-american-150-taco/BestOf?oid=4101416', 'http://www.chicagoreader.com/chicago/best-tater-tots/BestOf?oid=4101417', 'http://www.chicagoreader.com/chicago/best-spinach-pie/BestOf?oid=4101419', 'http://www.chicagoreader.com/chicago/best-som-tam/BestOf?oid=4101420', 'http://www.chicagoreader.com/chicago/best-soundtracked-strawberry-shake/BestOf?oid=4101498', 'http://www.chicagoreader.com/chicago/best-newly-minted-if-loosely-defined-neighborhood-for-destination-dining/BestOf?oid=4101624', 'http://www.chicagoreader.com/chicago/best-hangout-after-a-day-at-rogers-parks-beaches/BestOf?oid=4101655', 'http://www.chicagoreader.com/chicago/best-few-square-blocks-to-eat-really-well-and-cheaply/BestOf?oid=4101657', 'http://www.chicagoreader.com/chicago/best-place-to-feel-bad-about-gentrification-but-good-about-your-meal/BestOf?oid=4101658', 'http://www.chicagoreader.com/chicago/best-restaurant-staff-bent-on-making-your-day/BestOf?oid=4101659', 'http://www.chicagoreader.com/chicago/best-one-woman-kitchen/BestOf?oid=4101660', 'http://www.chicagoreader.com/chicago/most-underrated-james-beard-award-winner/BestOf?oid=4101661', 'http://www.chicagoreader.com/chicago/best-new-food-blog/BestOf?oid=4101662', 'http://www.chicagoreader.com/chicago/best-food-blog-written-by-a-linguist/BestOf?oid=4101663', 'http://www.chicagoreader.com/chicago/best-place-to-eat-and-drink-for-a-good-cause/BestOf?oid=4101664', 'http://www.chicagoreader.com/chicago/best-csa-farmer-to-swoon-over/BestOf?oid=4101665', 'http://www.chicagoreader.com/chicago/best-canine-control-at-a-farmers-market/BestOf?oid=4101666', 'http://www.chicagoreader.com/chicago/best-craft-brewery-that-doesnt-exist-yet/BestOf?oid=4101667', 'http://www.chicagoreader.com/chicago/best-cocktail-book-club/BestOf?oid=4101668']\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "--------------------------------------------------------------------- \n",
      "\n",
      "------------- TRYING ON MANTA.COM ---------------- (failed attempts :( )"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://www.manta.com/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from selenium import webdriver\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "\n",
      "url =  \"http://www.manta.com/mb_35_B121A000_000/general_automotive_repair_shops\" \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(url)\n",
      "driver.set_window_position(0, 0)\n",
      "driver.set_window_size(100000, 200000)\n",
      "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
      "time.sleep(2) # wait to load\n",
      "\n",
      "# at this point, if you see the Firefox window that opened you will see the message\n",
      "\n",
      "# Anyway, if you manage to get pass trough that blocking, you could load BeautifulSoup this way: \n",
      "soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.get('href'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#content\n",
        "yeabdeucczswaaz.html\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from selenium import webdriver\n",
      "\n",
      "chromedriver = \"/Users/mudy/Downloads/chromedriver\"\n",
      "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
      "driver = webdriver.Chrome(chromedriver)\n",
      "driver.get(\"http://stackoverflow.com\")\n",
      "driver.quit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "-------------------Trying on AUTOMD.COM---------------------- (y)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"https://www.automd.com\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2, sys\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "BASE_URL= \"https://www.automd.com/shops/\"\n",
      "TEST_URL= \"https://www.automd.com/shops/AL/\"\n",
      "BASE_URL2 = \"https://www.automd.com\"\n",
      "\n",
      "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "req = urllib2.Request(BASE_URL,headers=hdr)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "states = []\n",
      "links = []\n",
      "mysection = soup.find_all(\"div\", { \"class\" : \"seo-list-area\" })\n",
      "\n",
      "for link in mysection:\n",
      "    lnkState1 = link.contents[1].find_all(\"a\", href=True)[12]['href']\n",
      "    print lnkState1\n",
      "    \n",
      "for item in mysection:\n",
      "    for i in range(13):\n",
      "        tmpState1 = item.contents[1].find_all(\"a\")[i].text  #12th element is the last element\n",
      "        lnkState1 = item.contents[1].find_all(\"a\", href=True)[i]['href']\n",
      "        states.append(tmpState1)\n",
      "        links.append(lnkState1)\n",
      "        \n",
      "        tmpState2 = item.contents[3].find_all(\"a\")[i].text  #12th element is the last element\n",
      "        lnkState2 = item.contents[3].find_all(\"a\", href=True)[i]['href']\n",
      "        states.append(tmpState2)\n",
      "        links.append(lnkState2)\n",
      "        \n",
      "        tmpState3 = item.contents[5].find_all(\"a\")[i].text  #12th element is the last element\n",
      "        lnkState3 = item.contents[5].find_all(\"a\", href=True)[i]['href']\n",
      "        states.append(tmpState3)\n",
      "        links.append(lnkState3)\n",
      "        \n",
      "    for j in range(11):\n",
      "        tmpState4 = item.contents[7].find_all(\"a\")[j].text  #10th element is the last element\n",
      "        lnkState4 = item.contents[7].find_all(\"a\", href=True)[j]['href']\n",
      "        links.append(lnkState4)\n",
      "        states.append(tmpState4)\n",
      "\n",
      "print states, links"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/shops/IL/\n",
        "[u'Alabama', u'Indiana', u'Nebraska', u'Alaska', u'Iowa', u'Nevada', u'Arizona', u'Kansas', u'New Hampshire', u'Arkansas', u'Kentucky', u'New Jersey', u'California', u'Louisiana', u'New Mexico', u'Colorado', u'Maine', u'New York', u'Connecticut', u'Maryland', u'North Carolina', u'Delaware', u'Massachusetts', u'North Dakota', u'Florida', u'Michigan', u'Ohio', u'Georgia', u'Minnesota', u'Oklahoma', u'Hawaii', u'Mississippi', u'Oregon', u'Idaho', u'Missouri', u'Pennsylvania', u'Illinois', u'Montana', u'Rhode Island', u'South Carolina', u'South Dakota', u'Tennessee', u'Texas', u'Utah', u'Vermont', u'Virginia', u'Washington', u'West Virginia', u'Wisconsin', u'Wyoming'] ['/shops/AL/', '/shops/IN/', '/shops/NE/', '/shops/AK/', '/shops/IA/', '/shops/NV/', '/shops/AZ/', '/shops/KS/', '/shops/NH/', '/shops/AR/', '/shops/KY/', '/shops/NJ/', '/shops/CA/', '/shops/LA/', '/shops/NM/', '/shops/CO/', '/shops/ME/', '/shops/NY/', '/shops/CT/', '/shops/MD/', '/shops/NC/', '/shops/DE/', '/shops/MA/', '/shops/ND/', '/shops/FL/', '/shops/MI/', '/shops/OH/', '/shops/GA/', '/shops/MN/', '/shops/OK/', '/shops/HI/', '/shops/MS/', '/shops/OR/', '/shops/ID/', '/shops/MO/', '/shops/PA/', '/shops/IL/', '/shops/MT/', '/shops/RI/', '/shops/SC/', '/shops/SD/', '/shops/TN/', '/shops/TX/', '/shops/UT/', '/shops/VT/', '/shops/VA/', '/shops/WA/', '/shops/WV/', '/shops/WI/', '/shops/WY/']\n"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_links = []\n",
      "for eachlink in links:\n",
      "    tmpLink = 'https://www.automd.com' + str(eachlink)\n",
      "    full_links.append(tmpLink)\n",
      "print full_links\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['https://www.automd.com/shops/AL/', 'https://www.automd.com/shops/IN/', 'https://www.automd.com/shops/NE/', 'https://www.automd.com/shops/AK/', 'https://www.automd.com/shops/IA/', 'https://www.automd.com/shops/NV/', 'https://www.automd.com/shops/AZ/', 'https://www.automd.com/shops/KS/', 'https://www.automd.com/shops/NH/', 'https://www.automd.com/shops/AR/', 'https://www.automd.com/shops/KY/', 'https://www.automd.com/shops/NJ/', 'https://www.automd.com/shops/CA/', 'https://www.automd.com/shops/LA/', 'https://www.automd.com/shops/NM/', 'https://www.automd.com/shops/CO/', 'https://www.automd.com/shops/ME/', 'https://www.automd.com/shops/NY/', 'https://www.automd.com/shops/CT/', 'https://www.automd.com/shops/MD/', 'https://www.automd.com/shops/NC/', 'https://www.automd.com/shops/DE/', 'https://www.automd.com/shops/MA/', 'https://www.automd.com/shops/ND/', 'https://www.automd.com/shops/FL/', 'https://www.automd.com/shops/MI/', 'https://www.automd.com/shops/OH/', 'https://www.automd.com/shops/GA/', 'https://www.automd.com/shops/MN/', 'https://www.automd.com/shops/OK/', 'https://www.automd.com/shops/HI/', 'https://www.automd.com/shops/MS/', 'https://www.automd.com/shops/OR/', 'https://www.automd.com/shops/ID/', 'https://www.automd.com/shops/MO/', 'https://www.automd.com/shops/PA/', 'https://www.automd.com/shops/IL/', 'https://www.automd.com/shops/MT/', 'https://www.automd.com/shops/RI/', 'https://www.automd.com/shops/SC/', 'https://www.automd.com/shops/SD/', 'https://www.automd.com/shops/TN/', 'https://www.automd.com/shops/TX/', 'https://www.automd.com/shops/UT/', 'https://www.automd.com/shops/VT/', 'https://www.automd.com/shops/VA/', 'https://www.automd.com/shops/WA/', 'https://www.automd.com/shops/WV/', 'https://www.automd.com/shops/WI/', 'https://www.automd.com/shops/WY/']\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for url in full_links:\n",
      "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "    req = urllib2.Request(url, headers=hdr)\n",
      "    page = urllib2.urlopen(req)\n",
      "    soup = BeautifulSoup(page)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "----------- LET'S DEFINE A STANDARD FUNCTION FOR SENDING REQUESTS TO ANY PAGE AND MAKING A BEAUTIFUL SOUP OBJECT --------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_requests(url):\n",
      "    BASE_URL = \"https://www.automd.com\"\n",
      "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "    req = urllib2.Request(url, headers=hdr)\n",
      "    page = urllib2.urlopen(req)\n",
      "    soup = BeautifulSoup(page)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "---------- GETTING ALL SHOP NAMES FROM A 'state' PAGE --- (and cleaning them up)  ---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "url = \"https://www.automd.com/shops/AL/\"\n",
      "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "req = urllib2.Request(url, headers=hdr)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "\n",
      "\n",
      "for url in full_links:  #full_links is a list containing url of the states\n",
      "    get_ShopNames(url) #This will give all the shop names from the requested page; for now it's only page 1\n",
      "    \n",
      "    \n",
      "def get_ShopNames(state_url): #'https://www.automd.com/shops/AL/', 'https://www.automd.com/shops/IN/', ...... etc..\n",
      "    get_requests(state_url)\n",
      "    shopNames = []\n",
      "    shopURL = []\n",
      "    nextPageURL = []\n",
      "\n",
      "    sectionShop = soup.find_all(\"a\", { \"class\" : \"js-shop-url\" })\n",
      "    for item in sectionShop:\n",
      "        #print item.text\n",
      "        shopURL.append(BASE_URL + str(item['href']) ) #getting links to all shops on each page\n",
      "        nestr = re.sub(r'[^a-zA-Z0-9& ]',r'',item.text)\n",
      "        if nestr.startswith('10'):\n",
      "            #print nestr\n",
      "            shopNames.append(nestr[3:]) #SHOP NAMES WERE STARTING WITH numbers(list), so I had to start the string after the number\n",
      "        else:\n",
      "            shopNames.append(nestr[2:])\n",
      "\n",
      "    return shopURL, shopNames"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Pep Boys Pelham', u'Pep Boys Foley', u'Pep Boys Homewood', u'Pep Boys Montgomery', u'15th St Automotive', u'431 Auto and Body Repair', u'78 Alignment Services', u'82 Auto Parts & Wrecker Service', u'A & A Automotive', u'A & A Body Shop']\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### testing ### DELETE WHEN DONE: \n",
      "\n",
      "import re\n",
      "url = \"https://www.automd.com/shops/AL/\"\n",
      "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "req = urllib2.Request(url, headers=hdr)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "\n",
      "#get_requests(state_url)\n",
      "shopNames = []\n",
      "shopURL = []\n",
      "nextPageURL = []\n",
      "BASE_URL = \"https://www.automd.com\"\n",
      "#sectionShop = soup.findAll(\"a\", { \"class\" : \"js-shop-url\" }) # I can't do this now, since I won't know if shop is a standardShop\n",
      "#or a AutoMD IQ shop\n",
      "\n",
      "sectionShop = soup.findAll(\"li\", { \"class\" : \"mod-standard-shop\" })\n",
      "\n",
      "for item in sectionShop:\n",
      "    \n",
      "    print item.contents[0].find_all(\"a\", href=True)\n",
      "    shopURL.append(BASE_URL + str(item['href']) ) \n",
      "    nestr = re.sub(r'[^a-zA-Z0-9& ]',r'',item.text)\n",
      "    if nestr.startswith('10'):\n",
      "        #print nestr\n",
      "        shopNames.append(nestr[3:]) #SHOP NAMES WERE STARTING WITH numbers(list), so I had to start the string after the number\n",
      "    else:\n",
      "        shopNames.append(nestr[2:])\n",
      "\n",
      "print shopURL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'href'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-256-e12df717ae83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msectionShop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mshopURL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-zA-Z0-9& ]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/mudy/anaconda/lib/python2.7/site-packages/bs4/element.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n\u001b[1;32m    904\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'href'"
       ]
      }
     ],
     "prompt_number": 256
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#quick examples for string replacing\n",
      "import re\n",
      "strs = \"how much for the maple syrup? $20.99? That's & ricidulous!!!1\"\n",
      "print strs\n",
      "nstr = re.sub(r'[?|$|.|!]',r'',strs)\n",
      "print nstr\n",
      "nestr = re.sub(r'[^a-zA-Z ]',r'',nstr)\n",
      "print nestr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "how much for the maple syrup? $20.99? That's & ricidulous!!!1\n",
        "how much for the maple syrup 2099 That's & ricidulous1\n",
        "how much for the maple syrup  Thats  ricidulous\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "---------  GET REQUEST FOR EVERY PAGE NUMBER ---------------------- \\\n",
      "1. How many pages in each city listings \\\n",
      "2. Get the Url of all the pages and send a get request again \\\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2, sys\n",
      "from bs4 import BeautifulSoup\n",
      "#shops in alabama for now\n",
      "TEST_URL= \"https://www.automd.com/shops/AL/\"\n",
      "BASE_URL = \"https://www.automd.com\"\n",
      "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "req = urllib2.Request(TEST_URL,headers=hdr)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "\n",
      "pageNumItem = soup.find_all(\"div\", { \"class\" : \"pagination\" })\n",
      "\n",
      "lnks = []\n",
      "urlOfEachShop = []\n",
      "\n",
      "for num in pageNumItem:\n",
      "    dataPage = num.contents[0].find_all(\"a\", href=True)\n",
      "    for i in range(len(dataPage)-1): #pages 2 to 9\n",
      "        print dataPage[i]['href']\n",
      "        lnks.append(dataPage[i]['href'])\n",
      "    print len(dataPage)  # Now I know how mnay pages are \n",
      "    #print dataPage[1]['href']\n",
      "    \n",
      "fullPage_links = []\n",
      "for eachlink in lnks:\n",
      "    tmpLink = BASE_URL + str(eachlink)\n",
      "    fullPage_links.append(tmpLink)\n",
      "#print fullPage_links\n",
      "\n",
      "for pageUrl in fullPage_links:\n",
      "    print page\n",
      "    #get_requests(pageUrl)  #get_requests(url) is a function defined above for sending requests to the specified page\n",
      "    print get_ShopNames(pageUrl) #get_ShopNames function already contains a get_requests function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/shops/AL/?page=2\n",
        "/shops/AL/?page=3\n",
        "/shops/AL/?page=4\n",
        "/shops/AL/?page=5\n",
        "/shops/AL/?page=6\n",
        "/shops/AL/?page=7\n",
        "/shops/AL/?page=8\n",
        "/shops/AL/?page=9\n",
        "/shops/AL/?page=10\n",
        "10\n",
        "<addinfourl at 4670001736 whose fp = <socket._fileobject object at 0x116c79850>>\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'get_ShopNames' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-240-e55f0ef72179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#get_requests(pageUrl)  #get_requests(url) is a function defined above for sending requests to the specified page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mget_ShopNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageUrl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get_ShopNames function already contains a get_requests function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'get_ShopNames' is not defined"
       ]
      }
     ],
     "prompt_number": 240
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "ISSUE 1: SOME SHOPS ON A WEBPAGE ARE 'AUTOMD' IQ shops and some are 'standard shops'. BOTH HAVE A DIFFERENT STRUCTURE(HTML) \\\n",
      "    HENCE THE SCRIPT TO SCRAP THE DATA IS DIFFERENT"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TESTING -- DELETE WHEN DONE !!!!!!!!!!!!!!\n",
      "import urllib2, sys\n",
      "from bs4 import BeautifulSoup\n",
      "import copy\n",
      "#shops in alabama for now\n",
      "TEST_URL= \"https://www.automd.com/shops/AL/\"\n",
      "BASE_URL = \"https://www.automd.com\"\n",
      "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "req = urllib2.Request(TEST_URL,headers=hdr)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "\n",
      "standardShopNames = []\n",
      "standardShopURL = []\n",
      "IQshopNames = []\n",
      "IQshopsURL = []\n",
      "AllshopNames = []\n",
      "AllshopURL = []\n",
      "\n",
      "standardShop = soup.findAll(\"li\", { \"class\" : \"mod-standard-shop\" })\n",
      "Allshops = soup.findAll(\"li\", {\"class\" : \"listings-row\"})\n",
      "#print Allshops\n",
      "\n",
      "sectionShop = soup.find_all(\"a\", { \"class\" : \"js-shop-url\" })\n",
      "\n",
      "for item in sectionShop:\n",
      "    \n",
      "    #print item.contents[0].find_all(\"a\", href=True)\n",
      "    #shopURL.append(BASE_URL + str(item['href']) ) \n",
      "    nestr = re.sub(r'[^a-zA-Z0-9& ]',r'',item.text)\n",
      "    #print nestr\n",
      "    if nestr.startswith('10'):\n",
      "        #print nestr\n",
      "        AllshopNames.append(nestr[3:]) #SHOP NAMES WERE STARTING WITH numbers(list), so I had to start the string after the number\n",
      "    else:\n",
      "        AllshopNames.append(nestr[2:])\n",
      "\n",
      "for item2 in standardShop:\n",
      "        relurl = item2.find_all(\"a\", href=True)[0]['href']\n",
      "        relnames = item2.find_all(\"a\")[0].text\n",
      "        nestr = re.sub(r'[^a-zA-Z0-9& ]',r'',relnames)\n",
      "        if nestr.startswith('10'):\n",
      "        #print nestr\n",
      "            standardShopNames.append(nestr[3:]) #SHOP NAMES WERE STARTING WITH numbers(list), so I had to start the string after the number\n",
      "        else:\n",
      "            standardShopNames.append(nestr[2:])\n",
      "        standardShopURL.append(BASE_URL + str(relurl)) #full url\n",
      "\n",
      "for nam in AllshopNames:\n",
      "    if nam not in standardShopNames:\n",
      "        IQshopNames.append(nam)\n",
      "\n",
      "print \"Iqshops are:\", IQshopNames\n",
      "print \"standardshops are:\", standardShopNames"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iqshops are: [u'Pep Boys Pelham', u'Pep Boys Foley', u'Pep Boys Homewood', u'Pep Boys Montgomery']\n",
        "standardshops are: [u'15th St Automotive', u'431 Auto and Body Repair', u'78 Alignment Services', u'82 Auto Parts & Wrecker Service', u'A & A Automotive', u'A & A Body Shop']\n"
       ]
      }
     ],
     "prompt_number": 426
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "-------------- GET REQUEST FOR EACH SHOP; getting inside each individual shop e.g https://www.automd.com/shops/pep-boys-pelham_444508/ ---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for now let's take one ShopURL:  https://www.automd.com/shops/pep-boys-pelham_444508/ \n",
      "\n",
      "ShopURL = \"https://www.automd.com/shops/pep-boys-pelham_444508/\"\n",
      "def get_ShopInfo(ShopURL):\n",
      "    get_requests(ShopURL)\n",
      "    #what do I need from each shop? \n",
      "    contactNumber = []\n",
      "    shopEmail = []\n",
      "    shopWebSite = []\n",
      "    shopServices = []\n",
      "    shopAddress = []\n",
      "    yelp_reviews = []\n",
      "    \n",
      "    review = soup.find_all(\"div\", {\"class\" : \"yelp-review-area\"})\n",
      "    #for rev in review:\n",
      "    #    print rev.contents\n",
      "    address = soup.find_all(\"div\", {\"class\" : \"shop-profile-info-content-address\" })\n",
      "    for addr in address:\n",
      "        print addr.contents\n",
      "        \n",
      "if __name__ == '__main__':\n",
      "    get_ShopInfo(ShopURL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ShopURL = \"https://www.automd.com/shops/pep-boys-pelham_444508/\"\n",
      "\n",
      "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "req = urllib2.Request(ShopURL, headers=hdr)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "#what do I need from each shop? \n",
      "shopName = []\n",
      "contactNumber = []\n",
      "shopEmail = []\n",
      "shopWebSite = []\n",
      "shopServices = []\n",
      "shopAddress = []\n",
      "yelp_rating = []\n",
      "    \n",
      "details = soup.find_all(\"div\", {\"class\" : \"shop-profile-info-content\"})\n",
      "for info in details:\n",
      "    info.contents[3].find_all(\"meta\")[0]['data-rating']\n",
      "    \n",
      "    \n",
      "    shopName.append(info.contents[1].text)\n",
      "    shopAddress.append(info.contents[5].text)\n",
      "    contactNumber.append(info.contents[7].text)\n",
      "    yelp_rating.append(info.contents[3].find_all(\"meta\")[0]['data-rating'])\n",
      "    \n",
      "    \n",
      "address = soup.find_all(\"div\", {\"class\" : \"shop-profile-info-content-address\" })\n",
      "\n",
      "print yelp_rating"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['5']\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}